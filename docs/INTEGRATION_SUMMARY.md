# ğŸ¯ Resumo: Como Integrar Sites de Leiloeiros

## âœ… O Que Foi Criado

Implementei um **sistema completo de scraping** para integrar sites de leiloeiros:

### ğŸ“¦ Arquivos Criados:

1. **`src/lib/scraping/base-scraper.ts`** - Classe base com mÃ©todos Ãºteis
2. **`src/lib/scraping/scrapers/sodre-santoro.ts`** - Scraper de exemplo
3. **`src/lib/scraping/index.ts`** - Orquestrador principal
4. **`src/lib/scraping/utils.ts`** - FunÃ§Ãµes auxiliares (calculateDealScore, etc.)
5. **`src/app/api/cron/scrape/route.ts`** - API endpoint atualizada
6. **`docs/SCRAPING_GUIDE.md`** - Guia completo (LEIA ESTE!)

---

## ğŸš€ Como Funciona

### 1. Arquitetura

```
Cron Job (Vercel)
      â†“
 runAllScrapers()
      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ SodrÃ© Santoro   â”‚ â”€â”€â†’ Puppeteer â”€â”€â†’ Site Leiloeiro
  â”‚ Superbid        â”‚ â”€â”€â†’ Extrai dados
  â”‚ LeilÃµes VIP     â”‚ â”€â”€â†’ Busca FIPE
  â”‚ [... outros]    â”‚ â”€â”€â†’ Calcula Score
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”€â”€â†’ Salva Supabase
```

### 2. Classe Base (BaseScraper)

Fornece mÃ©todos prontos:

```typescript
// Extrair texto de elementos
await this.safeExtractText('.titulo');

// Extrair atributo
await this.safeExtractAttribute('img', 'src');

// Delay aleatÃ³rio (anti-detecÃ§Ã£o)
await this.randomDelay(1000, 3000);

// Parse de preÃ§os, anos, etc.
this.parsePrice('R$ 45.000,00'); // â†’ 45000
this.parseYear('2022/2023'); // â†’ { manufacture: 2022, model: 2023 }
```

### 3. Scraper EspecÃ­fico (Exemplo: SodrÃ© Santoro)

```typescript
export class SodreSantoroScraper extends BaseScraper {
  constructor() {
    super('SodrÃ© Santoro');
  }

  async scrapeVehicles(): Promise<VehicleData[]> {
    // 1. Acessa site do leiloeiro
    // 2. Extrai lista de veÃ­culos
    // 3. Para cada veÃ­culo, extrai detalhes
    // 4. Retorna array de VehicleData
  }
}
```

---

## ğŸ“ Como Adicionar um Novo Leiloeiro (Passo a Passo)

### Passo 1: Inspecionar o Site

1. Abra o site do leiloeiro no Chrome
2. Pressione `F12` (DevTools)
3. Use o **Seletor de Elementos** (Ã­cone de seta)
4. Clique nos elementos e veja os seletores CSS:
   - `.vehicle-card` (card do veÃ­culo)
   - `.price` (preÃ§o)
   - `h3.title` (tÃ­tulo)
   - `img.photo` (imagem)

### Passo 2: Criar o Scraper

Crie `src/lib/scraping/scrapers/novo-leiloeiro.ts`:

```typescript
import { BaseScraper, VehicleData } from '../base-scraper';

export class NovoLeiloeiro extends BaseScraper {
  private readonly baseUrl = 'https://site-leiloeiro.com.br';
  
  constructor() {
    super('Nome do Leiloeiro');
  }

  async scrapeVehicles(): Promise<VehicleData[]> {
    if (!this.page) throw new Error('PÃ¡gina nÃ£o inicializada');

    const vehicles: VehicleData[] = [];

    // 1. Navegar para o site
    await this.page.goto(`${this.baseUrl}/veiculos`, {
      waitUntil: 'networkidle2',
    });

    // 2. Aguardar cards carregarem
    await this.page.waitForSelector('.vehicle-card');

    // 3. Extrair URLs dos veÃ­culos
    const links = await this.page.evaluate(() => {
      const cards = document.querySelectorAll('.vehicle-card a');
      return Array.from(cards).map(a => (a as HTMLAnchorElement).href);
    });

    // 4. Processar cada veÃ­culo
    for (const link of links.slice(0, 20)) { // Limite de 20 para teste
      try {
        await this.page.goto(link);

        // Extrair dados
        const title = await this.safeExtractText('h1.title');
        const priceText = await this.safeExtractText('.price');
        const imageUrl = await this.safeExtractAttribute('img.photo', 'src');

        // Processar
        const price = this.parsePrice(priceText);
        const { brand, model } = this.parseTitleForBrandModel(title);

        vehicles.push({
          external_id: link.split('/').pop() || '',
          title,
          brand,
          model,
          current_bid: price,
          state: 'SP', // Extrair do site
          city: 'SÃ£o Paulo', // Extrair do site
          vehicle_type: 'Carro', // Detectar automaticamente
          original_url: link,
          thumbnail_url: imageUrl,
        });

        await this.randomDelay(1000, 2000); // Delay entre veÃ­culos
      } catch (error) {
        console.error(`Erro em ${link}:`, error);
      }
    }

    return vehicles;
  }

  // MÃ©todo auxiliar para extrair marca/modelo
  private parseTitleForBrandModel(title: string) {
    // Implemente sua lÃ³gica aqui
    return { brand: 'Chevrolet', model: 'Onix' };
  }
}
```

### Passo 3: Registrar no Orquestrador

Em `src/lib/scraping/index.ts`:

```typescript
import { NovoLeiloeiro } from './scrapers/novo-leiloeiro';

const scrapers = [
  new SodreSantoroScraper(),
  new NovoLeiloeiro(), // <-- Adicione aqui
];
```

### Passo 4: Cadastrar no Banco

Execute no Supabase SQL Editor:

```sql
INSERT INTO auctioneers (name, slug, website_url, scrape_frequency_hours)
VALUES ('Nome do Leiloeiro', 'nome-leiloeiro', 'https://site.com.br', 12);
```

### Passo 5: Testar

```bash
# Adicione ao .env.local:
CRON_SECRET="teste-123"

# Teste via API:
curl -H "Authorization: Bearer teste-123" http://localhost:3001/api/cron/scrape
```

---

## ğŸ§ª Testando Localmente

### OpÃ§Ã£o 1: Via Script

Crie `test-scraping.ts`:

```typescript
import { SodreSantoroScraper } from './src/lib/scraping/scrapers/sodre-santoro';

async function test() {
  const scraper = new SodreSantoroScraper();
  const vehicles = await scraper.run();
  console.log(`âœ… Encontrados ${vehicles.length} veÃ­culos`);
  console.log(vehicles[0]); // Ver primeiro veÃ­culo
}

test();
```

Execute:

```bash
npx tsx test-scraping.ts
```

### OpÃ§Ã£o 2: Via API

```bash
# Terminal 1: Iniciar servidor
npm run dev

# Terminal 2: Chamar API
curl -H "Authorization: Bearer teste-123" http://localhost:3001/api/cron/scrape
```

---

## â° Configurando Scraping AutomÃ¡tico (Vercel)

### Criar `vercel.json`:

```json
{
  "crons": [
    {
      "path": "/api/cron/scrape",
      "schedule": "0 */6 * * *"
    }
  ]
}
```

**Schedules comuns:**
- `0 */6 * * *` = A cada 6 horas
- `0 */12 * * *` = A cada 12 horas
- `0 9 * * *` = Todo dia Ã s 9h
- `0 0,12 * * *` = Ã€s 00:00 e 12:00

### Configurar VariÃ¡veis no Vercel:

1. VÃ¡ em **Settings** â†’ **Environment Variables**
2. Adicione:
   - `CRON_SECRET` = gere um secret forte
   - `SUPABASE_SERVICE_ROLE_KEY` = sua key do Supabase

---

## ğŸ“Š Monitoramento

### Ver Logs no Vercel:

1. VÃ¡ em **Deployments**
2. Clique no deployment atual
3. Clique em **Functions** â†’ **api/cron/scrape**
4. Veja os logs de execuÃ§Ã£o

### Ver Logs no Supabase:

```sql
-- Ãšltimas execuÃ§Ãµes
SELECT * FROM scraping_logs
ORDER BY started_at DESC
LIMIT 10;

-- EstatÃ­sticas
SELECT 
  a.name,
  COUNT(*) as executions,
  SUM(sl.vehicles_created) as total_created,
  AVG(sl.execution_time_ms) as avg_time_ms
FROM scraping_logs sl
INNER JOIN auctioneers a ON sl.auctioneer_id = a.id
WHERE sl.started_at > NOW() - INTERVAL '7 days'
GROUP BY a.name;
```

---

## âš ï¸ IMPORTANTE: Ã‰tica de Scraping

### âœ… FaÃ§a:

- Use delays entre requisiÃ§Ãµes
- Respeite robots.txt
- Use User-Agent realista
- Limite a quantidade (500 veÃ­culos por execuÃ§Ã£o)
- FaÃ§a cache de dados
- Execute em horÃ¡rios de baixo trÃ¡fego

### âŒ NÃ£o FaÃ§a:

- Scraping agressivo (muitas requisiÃ§Ãµes por segundo)
- Ignorar bloqueios do site
- Armazenar dados sensÃ­veis sem permissÃ£o
- Fazer scraping 24/7 sem pausas

---

## ğŸ› ï¸ Ferramentas Ãšteis

### Chrome DevTools

- **Elements**: Ver estrutura HTML
- **Network**: Ver requisiÃ§Ãµes
- **Console**: Testar seletores JavaScript

### Testar Seletores CSS:

No console do Chrome:

```javascript
document.querySelector('.vehicle-card'); // Um elemento
document.querySelectorAll('.vehicle-card'); // Todos elementos
```

### Gerar Cron Secret:

```bash
openssl rand -hex 32
```

---

## ğŸ“š DocumentaÃ§Ã£o Completa

Veja o guia detalhado em: **`docs/SCRAPING_GUIDE.md`**

---

## âœ… Checklist RÃ¡pido

- [ ] Leu `docs/SCRAPING_GUIDE.md`
- [ ] Inspecionou site do leiloeiro
- [ ] Criou scraper em `scrapers/`
- [ ] Testou localmente
- [ ] Cadastrou leiloeiro no banco
- [ ] Registrou no orquestrador
- [ ] Configurou `CRON_SECRET`
- [ ] Testou via API
- [ ] Criou `vercel.json`
- [ ] Fez deploy
- [ ] Monitorou primeira execuÃ§Ã£o

---

## ğŸ¯ PrÃ³ximos Passos

1. **Adapte o scraper de exemplo** (`sodre-santoro.ts`) para um leiloeiro real
2. **Teste localmente** com poucos veÃ­culos (20-50)
3. **Valide os dados** no banco de dados
4. **Adicione mais leiloeiros** usando o mesmo padrÃ£o
5. **Configure cron job** no Vercel
6. **Monitore** execuÃ§Ãµes e ajuste conforme necessÃ¡rio

---

**Pronto! VocÃª tem tudo que precisa para integrar qualquer leiloeiro! ğŸš€**

DÃºvidas? Consulte `docs/SCRAPING_GUIDE.md` ou abra uma issue no GitHub.

